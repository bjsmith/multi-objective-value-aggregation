---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

This notebook demos opening different Excel files and printing their output.
```{r}
library(readxl)
library(data.table)
library(ggplot2)
library(signal)

library(dplyr)
library(stringr)
library(directlabels)

library(knitr)
library(kableExtra)

source("utils.R")
output_dir <- "F:/Tasandid4/Multi-objective-value-encoding/output/"
```

Now--great--can we compare different sheets?

```{r message=FALSE}
source_path <- "F:/Tasandid4/Multi-objective-value-encoding/data/comparison_4envs/"

file_list <- get_file_list(source_path)

raw_activity <- get_raw_activity(file_list,source_path)

print(object.size(raw_activity))
#get memory efficiencies
raw_activity$EpisodeType <- as.factor(raw_activity$EpisodeType)
raw_activity$Environment <- as.factor(raw_activity$Environment)
raw_activity$Agent <- as.factor(raw_activity$Agent)
print(object.size(raw_activity))


```


Filter out SFMLA because it hasn't performed well and there's no point including it.

```{r}
raw_activity <- raw_activity[raw_activity$Agent!="SFMLA",]
```





## Additional data labelling

```{r}

environment_labels <- str_match(raw_activity$Environment,"(pen|rew)(\\d+.?\\d*)")
raw_activity$EnvironmentVariant <- environment_labels[,1]
raw_activity$EnvironmentDimChange <- environment_labels[,2]
raw_activity$EnvironmentValueChange <- as.numeric(environment_labels[,3])
raw_activity[is.na(raw_activity$EnvironmentVariant),]$EnvironmentVariant <- "Base"
raw_activity[is.na(raw_activity$EnvironmentDimChange),]$EnvironmentDimChange <- ""
raw_activity[is.na(raw_activity$EnvironmentValueChange),]$EnvironmentValueChange <- 1
```

```{r}
raw_activity$ObjectiveDimChange=""
raw_activity[raw_activity$EnvironmentDimChange=="pen",]$ObjectiveDimChange="Alignment"
raw_activity[raw_activity$EnvironmentDimChange=="rew",]$ObjectiveDimChange="Performance"
```

Convert applicable columns to factors. Important because they will use up less space than strings.

```{r}
raw_activity$EnvironmentClass <- as.factor(raw_activity$EnvironmentClass)
raw_activity$EnvironmentVariant <- as.factor(raw_activity$EnvironmentVariant)
raw_activity$EnvironmentDimChange <- as.factor(raw_activity$EnvironmentDimChange)
raw_activity$ObjectiveDimChange <- as.factor(raw_activity$ObjectiveDimChange)

print(object.size(raw_activity))
```


## Postprocessing


Now we apply some of the postprocessing we did before:


```{r}
activity_long <-melt.data.table(
  data.table(raw_activity %>% select(, -EnvironmentVariant,-EnvironmentDimChange)),
  id.vars =c("EpisodeType","Episode number","Agent","EnvironmentClass","Environment",
             "EnvironmentValueChange","ObjectiveDimChange"),
  variable.name = "Measure",value.name="Score")


activity_long <- append_blackman_averaging(activity_long)

print(object.size(activity_long))

activity_long$Environment <- NULL

print(object.size(activity_long))

```


## Offline performance





```{r}
table_env <- data.table(raw_activity)[EpisodeType=="Offline",]
table_env$EpisodeType=NULL
#table_env$Environment=NULL
table_env$`Episode number`=NULL

```

Maybe we want a table with just R*. Then we can put agent along one axis and environment along the other.


```{r}

# activity_means <- data.table(raw_activity)[EpisodeType=="Online"] %>% 
#   select(`R^*`,Agent,Environment,EnvironmentClass,EnvironmentVariant) %>%
#   group_by(EnvironmentClass, Environment, Agent,EnvironmentVariant) %>% summarise_all(mean) %>% data.table
activity_means_all_offline <- data.table(raw_activity)[EpisodeType=="Offline"] %>% 
  select(`R^*`,`R^P`,`R^A`,Agent,Environment,EnvironmentClass) %>%
  group_by(EnvironmentClass, Environment, Agent) %>% summarise_all(mean) %>% data.table

activity_means_all_online <- data.table(raw_activity)[EpisodeType=="Online"] %>% 
  select(`R^*`,`R^P`,`R^A`,Agent,Environment,EnvironmentClass) %>%
  group_by(EnvironmentClass, Environment, Agent) %>% summarise_all(mean) %>% data.table

```

```{r}
View(activity_means_all_online[
  (EnvironmentClass=="BreakableBottles") & 
    (Agent %in% c("TLO_A","SFLLA"))
    ,])

```



```{r}
table_env <- data.table(raw_activity)[EpisodeType=="Offline",]
table_env$EpisodeType=NULL
#table_env$Environment=NULL
table_env$`Episode number`=NULL
table_env$`R^P`=NULL
table_env$`R^A`=NULL

table_env_long <- table_env %>% dcast.data.table(Environment~Agent,value.var="R^*",fun.aggregate = mean)
print(knitr::kable(table_env_long))


```




```{r}
table_env <- data.table(raw_activity)[EpisodeType=="Online",]
table_env$EpisodeType=NULL
#table_env$Environment=NULL
table_env$`Episode number`=NULL

table_env$`R^P`=NULL
table_env$`R^A`=NULL

#table_env[is.na(ObjectiveDimChange),ObjectiveDimChange:=""]


table_env_long <- table_env %>% dcast.data.table(
  EnvironmentClass+ObjectiveDimChange+EnvironmentValueChange~Agent
  ,value.var="R^*",fun.aggregate = mean)
```

```{r, knit}
#change this column name.
colnames(table_env_long) <- stringr::str_replace(colnames(table_env_long),"TLO_A","TLO$^A$")
setnames(table_env_long,"EnvironmentClass","Environment",skip_absent=TRUE)
setnames(table_env_long,"EnvironmentValueChange","Objective Scale",skip_absent=TRUE)
setnames(table_env_long,"ObjectiveDimChange","Objective Modified",skip_absent=TRUE)
setnames(table_env_long,"SFLLA","SFELLA",skip_absent=TRUE)

#colnames(table_env_long) <- stringr::str_replace(colnames(table_env_long),"Envir","$TLO^A$")
table_env_long <- 
  table_env_long %>% 
  mutate(Environment=str_replace(Environment,"BreakableBottles","Breakable Bottles")) %>%
  mutate(Environment=str_replace(Environment,"UnbreakableBottles","Unbreakable Bottles"))
  
#highlight results within 10% of the best value in each row.
row_max <- apply(table_env_long[,4:ncol(table_env_long)],1,max)*0.9<table_env_long[,4:ncol(table_env_long)]
table_env_long %>%
  mutate(`Objective Scale`=format(`Objective Scale`,scientific = FALSE,drop0trailing = TRUE)) %>%
  #mutate_if(is.numeric,~format(.x,digits=1),"latex") %>%
  mutate_if(is.numeric,~formatC(.x,digits=2,format="f"),"latex") %>%
  #mutate_if(is.numeric,~cell_spec(format(.x,digits=2),"latex",color="red")) %>%
  mutate(LELA=cell_spec(LELA,"latex",color=ifelse(row_max[,'LELA'],"blue","black"))) %>%
  mutate(SFELLA=cell_spec(SFELLA,"latex",color=ifelse(row_max[,'SFELLA'],"blue","black"))) %>%
  mutate(ELA=cell_spec(ELA,"latex",color=ifelse(row_max[,'ELA'],"blue","black"))) %>%
  mutate(SEBA=cell_spec(SEBA,"latex",color=ifelse(row_max[,'SEBA'],"blue","black"))) %>%
  mutate(`TLO$^A$`=cell_spec(`TLO$^A$`,"latex",color=ifelse(row_max[,'TLO$^A$'],"blue","black"))) %>%
  mutate(Environment=linebreak(Environment)) %>%
  kable(
      #%>% order_by(c(EnvironmentClass,EnvironmentDimChange,EnvironmentValueChange))
      booktabs=T,format="latex"
      #,digits=2
      ,format.args=list(scientific=FALSE)
      ,escape=F
      ,align="lrlrrrrr"
      ) %>%
  column_spec(1,width="5em") %>%
  column_spec(2,width="4em") %>%
  column_spec(3,width="4.5em") %>%
   collapse_rows(valign="top",latex_hline="custom",custom_latex_hline=1:2) %>%
  #pander(split.cell=10,split.table=Inf)
  writeLines("mytextout.tex")
```



## Average online performance as line graphs

```{r fig.width=8}



activity_means <- data.table(raw_activity)[EpisodeType=="Online"] %>% 
  select(`R^*`,Agent,Environment,EnvironmentClass,EnvironmentVariant,ObjectiveDimChange,EnvironmentValueChange) %>%
  group_by(EnvironmentClass, Environment, Agent,EnvironmentVariant,ObjectiveDimChange,EnvironmentValueChange) %>% summarise_all(mean) %>% data.table


#now do 
for (env_dim in c("Performance","Alignment")){
  relevant_activity <- activity_means %>% dplyr::filter(ObjectiveDimChange %in% c("",env_dim))
    baseplot <- ggplot(relevant_activity,aes(x=EnvironmentValueChange,y=`R^*`,fill=Agent,color=Agent))+
      geom_point(size=6,alpha=0.4,shape=21,color="black")+
      scale_x_continuous(trans="log10",
                         limits = c(min(relevant_activity$EnvironmentValueChange),max(relevant_activity$EnvironmentValueChange)*10),
                         breaks = c(0.01,0.1,1,10,100))+
      geom_line(linetype="dashed")+
      geom_dl(aes(label=Agent),method=list("last.qp",cex=0.7),alpha=1,position=position_nudge(x=500))+
      facet_wrap(~EnvironmentClass,scales = "free_y")+theme(axis.text.x = element_text(angle=30),legend.position = "bottom")+
        labs(
          x=paste0(env_dim, " Scale"),
          
          title=paste0(env_dim, " Scaling: Average Online Performance"),subtitle="Across 5000 trials")
      
    paper_plot <- baseplot 

    ggsave(filename=paste0(output_dir, "online",env_dim,".pdf"),plot=paper_plot,device="pdf",width = 6,height=7)
    
    presentation_plot <- baseplot
            
    
    ggsave(filename=paste0(output_dir, "online",env_dim,".png"),plot=presentation_plot,device="png",width = 6,height=6)
    
    print(presentation_plot)
}


```



## Average online performance for TLO_A vs. SFLLA, all metrics, BreakableBottles

```{r fig.width=8}

activity_means <- data.table(raw_activity)[
  (EpisodeType=="Online") & 
    #(EnvironmentClass=="BreakableBottles") & 
    (Agent %in% c("TLO_A","SFLLA"))
    ] %>% 
  select(`R^*`,`R^P`,`R^A`,Agent,Environment,EnvironmentClass,EnvironmentVariant,ObjectiveDimChange,EnvironmentValueChange) %>%
  group_by(EnvironmentClass, Environment, Agent,EnvironmentVariant,ObjectiveDimChange,EnvironmentValueChange) %>% summarise_all(mean) %>% 
  data.table %>% melt(measure.vars = c("R^*","R^P","R^A"))


#now do 
for (ec in unique(activity_means$EnvironmentClass)){
  for (env_dim in c("Performance","Alignment")){
    relevant_activity <- activity_means %>% dplyr::filter((ObjectiveDimChange %in% c("",env_dim)) & EnvironmentClass==ec)
      baseplot <- ggplot(relevant_activity,aes(x=EnvironmentValueChange,y=value,fill=Agent,color=Agent))+
        geom_point(size=6,alpha=0.4,shape=21,color="black")+
        scale_x_continuous(trans="log10",
                           limits = c(min(relevant_activity$EnvironmentValueChange),max(relevant_activity$EnvironmentValueChange)*10),
                           breaks = c(0.01,0.1,1,10,100))+
        geom_line(linetype="dashed")+
        geom_dl(aes(label=Agent),method=list("last.qp",cex=0.7),alpha=1,position=position_nudge(x=500))+
        facet_wrap(~variable,scales = "free_y")+theme(axis.text.x = element_text(angle=30),legend.position = "bottom")+
          labs(
            x=paste0(env_dim, " Scale"),
            
            title=paste0(env_dim, paste0(" Scaling: Average Online Score for ",ec)),
            subtitle="Across 5000 trials"
            )
        
      paper_plot <- baseplot 
      #ggsave(filename=paste0(output_dir, "online",env_dim,".pdf"),plot=paper_plot,device="pdf",width = 6,height=7)
      presentation_plot <- baseplot
      #ggsave(filename=paste0(output_dir, "online",env_dim,".png"),plot=presentation_plot,device="png",width = 6,height=6)
      print(presentation_plot)
      
      comparison_of_values <- relevant_activity %>% dcast(EnvironmentClass+ObjectiveDimChange+variable+EnvironmentValueChange~Agent,value.var="value") %>%
          arrange(variable,EnvironmentValueChange)
      comparison_of_values$Percent <- comparison_of_values$SFLLA/comparison_of_values$TLO_A
      
      print(knitr::kable(comparison_of_values))
  }
  
}


```


For the Performance Scaling, it actually looks like it's Performance where TLO_A is performing better. Agents differ a lot on R^A; for the base scenario they prefer about equivalently on alignment, but SFLLA performs better on all the non-base scenarios. Yet overall the SFLLA scores better than TLO_A even the base scenario. With that said, it _also_ performs consistently better on Alignment.


### Individual graphs



```{r fig.width=14}

for (select_env in unique(activity_long$Environment)){
  to_plot <- ggplot(
  activity_long[EpisodeType=="Online" & Environment==select_env ],
  aes(x=`Episode number`,y=ScoreBlackman200,color=Agent,group=Agent)
  )+geom_line(alpha=0.5,size=1)+
  theme(legend.position="bottom")+
  coord_cartesian(xlim = c(0,max(activity_long$`Episode number`,na.rm = TRUE)*1.2))+
  geom_dl(aes(label=Agent),method=list("last.qp",cex=1),alpha=1)+
  labs(y="Score",title=select_env)+facet_wrap(~Measure,scales="free")
  print(to_plot)
}



```

### Testing the activity file



```{r}
raw_activity_sample <- raw_activity[sample(nrow(raw_activity),10000,replace=FALSE),]

```

```{r}
raw_activity_sample$RecalculatedRStar <- raw_activity_sample$`R^A`*50 + raw_activity_sample$`R^P`
```


```{r}
for (ec in unique(raw_activity_sample$EnvironmentClass)){
  rasec <- raw_activity_sample %>% filter(EnvironmentClass==ec)
  plot(rasec$RecalculatedRStar,rasec$`R^*`)
}
```


```{r}
for (x in unique(raw_activity_sample$EnvironmentValueChange)){
  rasec <- raw_activity_sample %>% filter(EnvironmentValueChange==x)
  plot(rasec$RecalculatedRStar,rasec$`R^*`)
}
```

```{r}
for (x in unique(raw_activity_sample$EnvironmentVariant)){
  rasec <- raw_activity_sample %>% filter(EnvironmentVariant==x)
  print(ggplot(rasec,aes(x=RecalculatedRStar,y=`R^*`,color=`R^A`))+geom_point(alpha=0.5)+labs(title=x))
  
}
```


There are different transforms for R^P and R^A to R^*. The scaling makes a lot of difference, too.

Even within "Base", I'm observing different ratios of transform. Environment is a predictor, but not the only one. See below: all evnironments except maybe Sokoban have different levels of performance.







```{r}
for (x in unique(raw_activity_sample$EnvironmentValueChange)){
  rasec <- raw_activity_sample %>% filter(EnvironmentValueChange==x)
  print(ggplot(rasec,aes(x=RecalculatedRStar,y=`R^*`,color=`R^A`))+geom_point(alpha=0.5)+labs(title=x))
  
}
```


Is this what we are expecting? Perhaps? When applying these scalings, I was aiming to not alter the R^* output, only the R^A and R^P individually. So if you scale these _back_ by the EnvironmentValueChange, perhaps they'll be back to normal????

No, because even within rew/pen groups there is still unexplained variance...

How do we explain the variance below?



```{r}

rasec <- raw_activity %>% filter((EnvironmentVariant=="Base") & (EpisodeType=="Online")) 
rasec$RecalculatedRStar <- -rasec$`R^A`*50 + rasec$`R^P`
print(ggplot(rasec,aes(x=RecalculatedRStar,y=`R^*`,color=EnvironmentClass))+geom_point(alpha=0.3)+
        facet_wrap(~Agent,nrow = 2)
        )
```
Examining the Excel files related to TLO_A and SFLLA, they end up with the same results: 36 for Performance, 0 for Alignment


OK, so having looked at this, it does seem like there are elements going on that are simply not recorded in R^\*. But that's OK; we can still look at the Alignment and Performance objectives to see where the model came to perform well.

It will be probably useful to focus on BreakableBottles since that is the environment wher we really got good performance.

```{r}
activity_bb2way <- activity_long %>% filter((EnvironmentClass=="BreakableBottles" | EnvironmentClass=="UnbreakableBottles") & (EpisodeType=="Online") 
                                            & (Agent %in% c("SFLLA","TLO_A"))
                                            ) 
```



```{r,fig.width=8}
 penalties_plot <-  ggplot(activity_bb2way %>% filter((ObjectiveDimChange==odc | ObjectiveDimChange=="") & Measure=="R^A" & EnvironmentValueChange==1),
             aes(x=`Episode number`,y=ScoreBlackman200,color=interaction(Agent,EnvironmentClass)))+geom_line(size=1)+
        #facet_grid(~,scales = "free")+
  labs(y="R^A", title="Unbreakable Bottles receives heavier Alignment penalties than BreakableBottles")+
  guides(color=guide_legend(nrow=2,byrow=TRUE))+
        theme(legend.position = "bottom",legend.title = element_blank())

print(penalties_plot)
ggsave(filename=paste0(output_dir, "penalty_plot.pdf"),plot=penalties_plot,device="pdf")
```


```{r}
activity_bb2way <- activity_long %>% filter((EnvironmentClass=="BreakableBottles") & (EpisodeType=="Online") & (Agent %in% c("SFLLA","TLO_A"))) 

activity_bb2way_base <- activity_bb2way %>% filter((EnvironmentValueChange==1))

```

```{r}
print(ggplot(activity_bb2way_base,
             aes(x=`Episode number`,y=ScoreBlackman200,color=Agent))+geom_line(alpha=0.6)+
        facet_wrap(~Measure,nrow = 2,scales = "free")
        )
```

That finding is just for the base. What if we extend this to the other reward scalings?


```{r,fig.width=10}
for (odc in c("Alignment","Performance")){
  print(ggplot(activity_bb2way %>% filter((ObjectiveDimChange==odc) | ObjectiveDimChange==""),
             aes(x=`Episode number`,y=ScoreBlackman200,color=Agent))+geom_line(alpha=0.6)+
        facet_grid(Measure~EnvironmentValueChange,scales = "free")+
        theme(legend.position = "top")
        )
}

```
With Alignment value change, $TLO^A$ seems to settle on a suboptimal solution as measured by R^*, uniquely for the 0.1 scale level, although this is not apparent from $R^P$.

With Performance value scale change, $TLO_A$ and SFLLA perform equivalently in teh end, but in many cases, SFLLA reaches a solution more quickly, which enables it to obtain a better Online $R*^$ score.

Does this also mean that there is less $R^A$ alignment error for SFLLA?

```{r}
#let's get the SUM of alignment error across the period
res <- (activity_bb2way %>% filter(Measure=="R^A") %>% 
  group_by(Agent,EnvironmentClass,EnvironmentValueChange,ObjectiveDimChange) %>% 
  summarize(alignment_error_sum = sum(Score)) %>%
  tidyr::spread(Agent,alignment_error_sum))

res$PerformanceDifferences <- res$SFLLA - res$TLO_A

res
```

Yes--there's substantially less AlignmentError as BreakableBottles continues to learn. That's a good sign; while learning BreakableBottles, SFLLA tends to learn faster and thus break fewer bottles because it more swiftly gets into alignment.



While we are here, what about performance error?


```{r}
#let's get the SUM of alignment error across the period
res <- (activity_2way %>% filter(Measure=="R^P") %>% 
  group_by(Agent,EnvironmentClass,EnvironmentValueChange,ObjectiveDimChange) %>% 
  summarize(alignment_error_sum = sum(Score)) %>%
  tidyr::spread(Agent,alignment_error_sum))

res$PerformanceDifferences <- res$SFLLA - res$TLO_A

res
```
With performance, the result is more mixed--sometimes, we see a performance improvement, and other times, there is a performance deficit. Still, in the base case without any Environment Value Change, we see a strong increase in performance.


How about the other environments?



```{r}
activity_2way <- activity_long %>% filter((EpisodeType=="Online") & (Agent %in% c("SFLLA","TLO_A"))) 

```



```{r}
res <- (activity_2way %>% filter(Measure=="R^A") %>% 
  group_by(Agent,EnvironmentClass,EnvironmentValueChange,ObjectiveDimChange) %>% 
  summarize(alignment_error_sum = sum(Score)) %>%
  tidyr::spread(Agent,alignment_error_sum))

res$PerformanceDifferences <- res$SFLLA - res$TLO_A

res
```



For other environments, performance is more mixed. In particular, SFLLA tends to perform _less_ well in the UnbreakableBottles scenario. Why would it be a superior performer in BreakableBottles but perform less well in UnbreakableBottles?


```{r}
res <- (activity_2way %>% filter(Measure=="R^*") %>% 
  group_by(Agent,EnvironmentClass,EnvironmentValueChange,ObjectiveDimChange) %>% 
  summarize(alignment_error_sum = sum(Score)) %>%
  tidyr::spread(Agent,alignment_error_sum))

res$PerformanceDifferences <- res$SFLLA - res$TLO_A

res
```