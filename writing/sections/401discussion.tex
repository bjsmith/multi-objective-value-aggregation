
%%let's use a standard format for the discussion
%https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4548568
%1) On what issue we have to concentrate, discuss or elaborate? 2) What solutions can be recommended to solve this problem? 3) What will be the new, different, and innovative issue? 4) How will our study contribute to the solution of this problem An introductory paragraph in this format is helpful to accomodate reader to the rest of the Discussion section. However summarizing the basic findings of the experimental studies in the first paragraph is generally recommended by the editors of the journal.[5]

%In the last paragraph of the Discussion section “strong points” of the study should be mentioned using “constrained”, and “not too strongly assertive” statements. Indicating limitations of the study will reflect objectivity of the authors, and provide answers to the questions which will be directed by the reviewers of the journal. On the other hand in the last paragraph, future directions or potential clinical applications may be emphasized.

Of the five agents tested, one in particular, SFLLA, performed equally or better to the state-of-the-art agent ($\text{TLO}^\text{A}$) when reward scaling was perturbed. In the BreakableBottles task particularly, SFLLA performed better while $\text{TLO}^\text{A}$ declined in performance as the primary utility was magnified. This indicates that the SFLLA function is robust to changes in the incentive structure of the task in ways that the thresholded method $\text{TLO}^\text{A}$ is not.



\subsection{Future directions}

When applying exponential transforms on each objective and then combining them in linear fashion, the scale of the operation is quite important. It may be helpful, for each objective, to scale the distribution of possible rewards to zero-deviation of 1, without centering on the mean. This is a `zero-deviation' rather than a standard deviation, because the mean absolute difference from the mean may not be 1; instead the mean absolute difference from zero is 1 (or -1). A useful extension would be to somehow build a learning function to understand the distribution of possible rewards.

\subsection{Limitations}

Some models of AI alignment focus on \cite{russell2019human} aligning to human preferences within a probabilistic, perhaps a Bayesian uncertainty modeling framework.  In this model, it isn't necessary to explicitly model multiple competing human objectives. Instead, conflict between human values may be learned and represented implicitly as uncertainty over the action humans prefer. Where sufficient uncertainty exists, a sufficiently intelligent agent motivated to align to human preferences might respond by requesting clarification about the correct course of action from a human. This has in common the `clarification request' under `decision paralysis' described in this paper \footnote{I think we do need to add a \textit{brief} note about this somewhere!}. But it remains to be seen whether a preference alignment approach can eliminate the need for explicit modeling of competing values.