\subsection{Results}
\footnote{probably want just one Sokoban in the Table;, need Roland to help decide which}


\begin{table}[t]
\footnotesize
  \caption{Performance over granularity levels relative to \tloA{}. Items are significantly different from \tloA{} when marked *$p<0.05$, ** $p <0.01$, *** $p<0.001$; arrows mark the direction of significant differences.}
  \label{tab:granularity_significance}
\begin{adjustbox}{width=\columnwidth}
\input{output/tables/significance_granularity_abridged.tex}
\end{adjustbox}
\end{table}


\begin{figure}
  %\centering
  %\includegraphics[width=\columnwidth]{output/onlinepen.pdf}
  \includegraphics[width=\columnwidth]{output/multirun_n100_pilot_granularityonline_RewGranularity.pdf}
  \includegraphics[width=\columnwidth]{output/multirun_n100_pilot_granularityonline_PenGranularity.pdf}
  \caption{Experiment 3: By creating granularity for our non-linear transform agents, we can simulate similarity with \tloA{}. \tloA{} can be modeled as a non-linear transform with very large granularity. As Primary and Alignment granularity increases, we become more similar to \tloA{} only in fine-tuning the sensitivity to changes across the scale. This generally worsens SFELLA performance, particularly in Primary objective granularity scaling.
  }
   \label{fig:exp3_main}

 \end{figure}
 
For SFELLA, as expected, \RStar{} performance declined as granularity increased. This was particularly noteable the BreakableBottles environment where it previously had a clear advantage over \tloA{} (Figure~\ref{fig:exp3_main}). For UnbreakableBottles, performance declined as Primary reward granularity increased, but actually marginally improved as penalty granularity was increased.% In the Doors environment, performance declined, from significantly better (??? Table~\ref{tab:granularity_significance}) than \tloA{} to much, much worse. In the Sokoban environment, as in Experiment 1, \tloA{} was the better performer and remained so.

The result confirms that where SFELLA performs well, it does so because it avoids `granularity' and is sensitive to changes in reward right across the scale. In contrast, \tloA{} is sometimes insensitive to changes that exceed its threshold. Where it is well tuned, it performs well, or even better, than other algorithms, but when not well-tuned, it performs less well.\footnote{should this be moved to the discussion?}





