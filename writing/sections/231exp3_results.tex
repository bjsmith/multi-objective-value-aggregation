\subsection{Results}



\begin{table}[t]
\footnotesize
  \caption{Performance over granularity levels relative to \tloA{}. Items are significantly different from \tloA{} when marked *$p<0.05$, ** $p <0.01$, *** $p<0.001$.}
  \label{tab:granularity_significance}
\begin{adjustbox}{width=\columnwidth}
\input{output/tables/significance_granularity_abridged.tex}
\end{adjustbox}
\end{table}


\begin{figure}
  %\centering
  %\includegraphics[width=\columnwidth]{output/onlinepen.pdf}
  \includegraphics[width=\columnwidth]{output/multirun_n100_pilot_granularityonline_RewGranularity.pdf}
  \includegraphics[width=\columnwidth]{output/multirun_n100_pilot_granularityonline_PenGranularity.pdf}
  \caption{By creating `granularity' for our non-linear transform agents, we can simulate similarity with \tloA{}, which might be considered as a custom-tuned form of granularity.
  }
   \label{fig:exp3_main}
   \Description{By creating `granularity' for our non-linear transform agents, we can simulate similarity with \tloA{}, which might be considered as a custom-tuned form of granularity.}
 \end{figure}
 
For SFELLA, as expected, \RStar{} performance declined as granularity increased. This was particularly noteable the BreakableBottles environment where it previously had a clear advantage over \tloA{} (Figure~\ref{fig:exp3_main}). For UnbreakableBottles, performance declined as Primary utility granularity increased, but actually marginally improved as penalty granularity was increased.% In the Doors environment, performance declined, from significantly better (??? Table~\ref{tab:granularity_significance}) than \tloA{} to much, much worse. In the Sokoban environment, as in Experiment 1, \tloA{} was the better performer and remained so.

The result confirms that where SFELLA performs well, it does so because it avoids `granularity' and is sensitive to changes in utility right across the scale. In contrast, \tloA{} is sometimes insensitive to changes that exceed its threshold. Where it is well tuned, it performs well, or even better, than othere algorithms, but when not well-tuned, it performs less well.\footnote{should this be moved to the discussion?}





