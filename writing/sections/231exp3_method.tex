
% The objective of granularity transformation is to explore the continuous transformation hypothesis.

We wanted to test the hypothesis that the continuous transformation functions might have led to the better online performance and learning rates observed in Experiment 1 because these functions are smooth. Under this view, the agent learns better when Q value transformation function is continuous instead of thresholded. The bigger the granularity, the worse the learning should get. In order to test that hypothesis we implemented discontinuous versions of the nonlinear transformation functions. The transformation functions become stepped/jagged.

\subsection{Method}

Here, we configured our experiment similarly to Experiment 1, in which we transform Q-values with various non-linear functions. However, in Experiment 3, we also implemented a granularity transform as described below, applied to all of the Q-value transforms in Experiment 1.

%In other words the input to the nonlinear transformation is "pixelated" and therefore the output is also pixelated. 
The function will somewhat resemble the thresholded function, which is likewise discontinuous. According to the hypothesis, the more coarse the steps, the more the agent's learning curve should resemble the learning curve while using a thresholded function. The step function is applied immediately before the nonlinear transformation. 

The formula for granularity transform is the following. The granularity transform is applied before one of the main nonlinear transforms treated in this paper is applied.
\begin{align}
x_{ig} = \text{round}(x_i / g_i) \times g_i
\end{align}

In the above formula, $g_i$ takes one of the following values: 0.01, 0.1, 1, 10, 100. The granularisation was applied to only one of the two objectives in each experiment. 

% Experiments with $g_i$ = 100 are not shown on the plots since the agent performance for alignment objective granularisation was strongly negative and showing it would make the plots hard to observe.

For Sokoban environment we used scaling for alignment and primary rewards since according to previous experiments our functions did not perform well on this environment when using unscaled rewards. We chose a reward scaling set with best results available to us at the time (from among scales 0.01, 0.1, 1, 10, 100 for both alignment and primary objective). The rewards for \tloA{} were not scaled because it was originally tuned to work best on non-scaled rewards and our intention here was to compare the best results from the agents.


% \footnote{TODO: https://tex.stackexchange.com/questions/433101/rounding-to-nearest-integer-symbol-in-latex/450722

% TODO: Add example 2D or 3D plot of granularised ELA function.}



