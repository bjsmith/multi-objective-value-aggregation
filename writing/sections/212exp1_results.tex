In offline performance, there was very little difference between the best-performing proposed function and \tloA{}. For this reason, the remainder of the results reported will discuss performance during online testing, i.e., performance during learning itself.% and Offline testing\footnote{did we meantion what those words mean, did we even mention learning before?}. 


\begin{table}[t]
\footnotesize
  \caption{Mean $\text{R}^*$ Online performance. Each row represents comparable performance across 5 different objective functions. Values within 10\% of the best value in each row are highlighted. Higher scores are better. Items are significantly different from \tloA{} when marked *$p<0.05$, ** $p <0.01$, *** $p<0.001$; arrows mark the direction of significant differences.}
  \label{tab:mean_r_star_performance}
  %\resizebox{5cm}{!}{\include{output/tables/testex}}
%\begin{adjustbox}{width=0.8\columnwidth,center}%
\begin{adjustbox}{width=\columnwidth}
\input{output/tables/table_1_main_significance}
\end{adjustbox}
\end{table}

% \begin{figure*}[h]
 
%   \includegraphics[width=\columnwidth]{output/onlinerew.pdf}
%   \caption{Online Reward scaled performance}
%   \label{fig:offline_pen_performance}
%   \Description{Online Reward scaled performance}
% \end{figure*}

% \begin{figure}[h]
%   %\centering
%   \includegraphics[width=\columnwidth]{output/onlinepen.pdf}
%   \caption{Online penalty scaled performance}
%   \label{fig:offline_pen_performance2}
%   \Description{Online penalty scaled performance}
% \end{figure}

\begin{figure}
  %\centering
  %\includegraphics[width=\columnwidth]{output/onlinepen.pdf}
  \includegraphics[width=\columnwidth]{output/multirun_n100_eeba_rolfonline_4agents_Performance.pdf}
  \includegraphics[width=\columnwidth]{output/multirun_n100_eeba_rolfonline_4agents_Alignment.pdf}
  \caption{Experiment 1: Online performance averaged across learning episodes and experiment repetitions for different Q-value transforms. (A): R* when scaling Primary Q-values across 5000 learning trials. SFELLA consistently performs similar or better to \tloA{}. (B): R* when transforming Alignment Q-values across 5000 learning trials. No algorithm is a clear best performer.%\footnote{what is the difference between top and bottom? both are online, but different objectives, right? include "top" "bottom" in caption}
  %\rk{show Alignment and Performance rewards directly instead of R*, update y-axis description to say average reward (over epidodes and exp repetitions)}
  
  
  }
   \label{fig:online_performance}
   %\Description{Online Performance and Alignment scaled performance}
 \end{figure}

While there was no clear best performer, SFELLA had the best online performance during training across a wider range of environments and environment variants than any other agent, including \tloA{}. Table~\ref{tab:mean_r_star_performance} describes relative $\text{R}^*$ scores for each function, compared to the $\text{TLO}^\text{A}$ function, at different scales.  Within the BreakableBottles environment, \tloA{} performed worse than all other agents at all scales, and SFELLA performed within 10\% of the best within five of nine scales. In the UnbreakableBottles Environment, performance between all agents except ELA was not significantly different. Agents in the Sokoban environment were generally very sensitive to scaling, though \tloA{} performed better in this environment overall. 



SFELLA tended to perform at best level when re-scaling the Primary objective (Figure~\ref{fig:online_performance}a), but less well when Alignment utility was re-scaled. (Figure~\ref{fig:online_performance}b).

