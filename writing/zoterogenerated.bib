
@inproceedings{dewey_learning_2011,
	title = {Learning what to value},
	booktitle = {International {Conference} on {Artificial} {General} {Intelligence}},
	publisher = {Springer},
	author = {Dewey, Daniel},
	year = {2011},
	pages = {309--314}
}


@book{russell2019human,
  title={Human compatible: Artificial intelligence and the problem of control},
  author={Russell, Stuart},
  year={2019},
  publisher={Penguin}
}

@article{vamplew_human-aligned_2018,
	title = {Human-aligned artificial intelligence is a multiobjective problem},
	volume = {20},
	number = {1},
	journal = {Ethics and Information Technology},
	author = {Vamplew, Peter and Dazeley, Richard and Foale, Cameron and Firmin, Sally and Mummery, Jane},
	year = {2018},
	note = {Publisher: Springer},
	pages = {27--40}
}

@article{ajmeri_designing_2018,
	title = {Designing ethical personal agents},
	volume = {22},
	number = {2},
	journal = {IEEE Internet Computing},
	author = {Ajmeri, Nirav and Guo, Hui and Murukannaiah, Pradeep K. and Singh, Munindar P.},
	year = {2018},
	note = {Publisher: IEEE},
	pages = {16--22}
}

@article{sotala_superintelligence_2017,
	title = {Superintelligence as a cause or cure for risks of astronomical suffering},
	volume = {41},
	number = {4},
	journal = {Informatica},
	author = {Sotala, Kaj and Gloor, Lukas},
	year = {2017}
}

@phdthesis{ajmeri_engineering_2019,
	type = {{PhD} {Thesis}},
	title = {Engineering {Multiagent} {Systems} for {Ethics} {Aware} and {Privacy} {Respecting} {Social} {Computing}},
	school = {North Carolina State University},
	author = {Ajmeri, Nirav},
	year = {2019}
}

@article{yampolskiy_artificial_2016,
	title = {Artificial intelligence safety and cybersecurity: {A} timeline of {AI} failures},
	shorttitle = {Artificial intelligence safety and cybersecurity},
	journal = {arXiv preprint arXiv:1610.07997},
	author = {Yampolskiy, Roman V. and Spellchecker, M. S.},
	year = {2016}
}

@article{yampolskiy_predicting_2019,
	title = {Predicting future {AI} failures from historic examples},
	journal = {foresight},
	author = {Yampolskiy, Roman V.},
	year = {2019},
	note = {Publisher: Emerald Publishing Limited}
}

@inproceedings{sarma_ai_2018,
	title = {{AI} {Safety} and {Reproducibility}: {Establishing} {Robust} {Foundations} for the {Neuropsychology} of {Human} {Values}},
	shorttitle = {{AI} {Safety} and {Reproducibility}},
	booktitle = {International {Conference} on {Computer} {Safety}, {Reliability}, and {Security}},
	publisher = {Springer},
	author = {Sarma, Gopal P. and Hay, Nick J. and Safron, Adam},
	year = {2018},
	pages = {507--512}
}

@inproceedings{ajmeri_elessar_2020,
	title = {Elessar: {Ethics} in {Norm}-{Aware} {Agents}.},
	shorttitle = {Elessar},
	booktitle = {{AAMAS}},
	author = {Ajmeri, Nirav and Guo, Hui and Murukannaiah, Pradeep K. and Singh, Munindar P.},
	year = {2020},
	pages = {16--24}
}

@inproceedings{rzepka_what_2017,
	title = {What people say? {Web}-based casuistry for artificial morality experiments},
	shorttitle = {What people say?},
	booktitle = {International {Conference} on {Artificial} {General} {Intelligence}},
	publisher = {Springer},
	author = {Rzepka, Rafal and Araki, Kenji},
	year = {2017},
	pages = {178--187}
}

@article{everitt_agi_2018,
	title = {{AGI} safety literature review},
	journal = {arXiv preprint arXiv:1805.01109},
	author = {Everitt, Tom and Lea, Gary and Hutter, Marcus},
	year = {2018}
}

@article{ajmeri_engineering_2018,
	title = {Engineering {Multiagent} {Systems} for {Ethics} and {Privacy}-{Aware} {Social} {Computing}.},
	author = {Ajmeri, Nirav},
	year = {2018}
}

@article{sarma_sarma_2018,
	title = {sarma},
	journal = {arXiv preprint arXiv:1811.03493},
	author = {Sarma, Gopal P. and Safron, Adam and Hay, Nick J.},
	year = {2018}
}

@article{potapov_universal_2014,
	title = {Universal empathy and ethical bias for artificial general intelligence},
	volume = {26},
	issn = {0952-813X},
	url = {https://doi.org/10.1080/0952813X.2014.895112},
	doi = {10.1080/0952813X.2014.895112},
	abstract = {Rational agents are usually built to maximise rewards. However, artificial general intelligence (AGI) agents can find undesirable ways of maximising any prior reward function. Therefore, value learning is crucial for safe AGI. We assume that generalised states of the world are valuable – not rewards themselves, and propose an extension of AIXI, in which rewards are used only to bootstrap hierarchical value learning. The modified AIXI agent is considered in the multi-agent environment, where other agents can be either humans or other ‘mature’ agents, the values of which should be revealed and adopted by the ‘infant’ AGI agent. A general framework for designing such empathic agent with ethical bias is proposed as an extension of the universal intelligence model as well. Moreover, we perform experiments in the simple Markov environment, which demonstrate feasibility of our approach to value learning in safe AGI.},
	number = {3},
	urldate = {2021-03-06},
	journal = {Journal of Experimental \& Theoretical Artificial Intelligence},
	author = {Potapov, Alexey and Rodionov, Sergey},
	month = jul,
	year = {2014},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/0952813X.2014.895112},
	keywords = {empathy, AIXI, multi-agent environment, representations, safe AGI},
	pages = {405--416}
}

@article{everitt_self-modification_2016,
	title = {Self-{Modification} of {Policy} and {Utility} {Function} in {Rational} {Agents}},
	url = {http://arxiv.org/abs/1605.03142},
	abstract = {Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and ﬁngers), will in principle have the ability to self-modify – for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may ﬁnd ways to change their goals to something more easily achievable, thereby ‘escaping’ the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modiﬁcation possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modiﬁcations and use the current utility function when evaluating the future.},
	language = {en},
	urldate = {2021-03-06},
	journal = {arXiv:1605.03142 [cs]},
	author = {Everitt, Tom and Filan, Daniel and Daswani, Mayank and Hutter, Marcus},
	month = may,
	year = {2016},
	note = {arXiv: 1605.03142},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Artificial General Intelligence (AGI) 2016}
}

@article{vamplew_potential-based_2021,
	title = {Potential-based multiobjective reinforcement learning approaches to low-impact agents for {AI} safety},
	volume = {100},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0952197621000336},
	doi = {10.1016/j.engappai.2021.104186},
	abstract = {The concept of impact-minimisation has previously been proposed as an approach to addressing the safety concerns that can arise from utility-maximising agents. An impact-minimising agent takes into account the potential impact of its actions on the state of the environment when selecting actions, so as to avoid unacceptable side-effects. This paper proposes and empirically evaluates an implementation of impactminimisation within the framework of multiobjective reinforcement learning. The key contributions are a novel potential-based approach to specifying a measure of impact, and an examination of a variety of non-linear action-selection operators so as to achieve an acceptable trade-off between achieving the agent’s primary task and minimising environmental impact. These experiments also highlight a previously unreported issue with noisy estimates for multiobjective agents using non-linear action-selection, which has broader implications for the application of multiobjective reinforcement learning.},
	language = {en},
	urldate = {2021-03-03},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Vamplew, Peter and Foale, Cameron and Dazeley, Richard and Bignold, Adam},
	month = apr,
	year = {2021},
	keywords = {AI safety, Low-impact agents, Multiobjective reinforcement learning, Potential-based rewards, Reward engineering, Safe reinforcement learning, Side-effects},
	pages = {104186}
}

@article{armstrong_low_2017,
	title = {Low {Impact} {Artificial} {Intelligences}},
	url = {http://arxiv.org/abs/1705.10720},
	abstract = {There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: defining a general concept of `low impact'. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of defining and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.},
	urldate = {2021-02-15},
	journal = {arXiv:1705.10720 [cs]},
	author = {Armstrong, Stuart and Levinstein, Benjamin},
	month = may,
	year = {2017},
	note = {arXiv: 1705.10720},
	keywords = {Computer Science - Artificial Intelligence}
}


@inproceedings{rolf_need_2020,
	title = {The {Need} for {MORE}: {Need} {Systems} as {Non}-{Linear} {Multi}-{Objective} {Reinforcement} {Learning}},
	shorttitle = {The {Need} for {MORE}},
	doi = {10.1109/ICDL-EpiRob48136.2020.9278062},
	abstract = {Both biological and artificial agents need to coordinate their behavior to suit various needs at the same time. Reconciling conflicts of different needs and contradictory interests such as self-preservation and curiosity is the central difficulty arising in the design and modelling of need and value systems. Current models of multi-objective reinforcement learning do either not provide satisfactory power to describe such conflicts, or lack the power to actually resolve them. This paper aims to promote a clear understanding of these limitations, and to overcome them with a theory-driven approach rather than ad hoc solutions. The first contribution of this paper is the development of an example that demonstrates previous approaches' limitations concisely. The second contribution is a new, non-linear objective function design, MORE, that addresses these and leads to a practical algorithm. Experiments show that standard RL methods fail to grasp the nature of the problem and ad-hoc solutions struggle to describe consistent preferences. MORE consistently learns a highly satisfactory solution that balances contradictory needs based on a consistent notion of optimality.},
	booktitle = {2020 {Joint} {IEEE} 10th {International} {Conference} on {Development} and {Learning} and {Epigenetic} {Robotics} ({ICDL}-{EpiRob})},
	author = {Rolf, M.},
	month = oct,
	year = {2020},
	note = {ISSN: 2161-9484},
	keywords = {Reinforcement learning, reinforcement learning, Biological system modeling, learning (artificial intelligence), Mathematical model, Planning, Task analysis, Collision avoidance, artificial agents, ad hoc solutions, ad-hoc solutions, biological agents, central difficulty, contradictory interests, contradictory needs, multiobjective reinforcement learning, multiple objectives, Need systems, nonlinear objective function design, reconciling conflicts, satisfactory power, satisfactory solution, self-preservation, Standards, value systems},
	pages = {1--8}
}

@article{ecoffet_reinforcement_2020,
	title = {Reinforcement {Learning} {Under} {Moral} {Uncertainty}},
	url = {http://arxiv.org/abs/2006.04734},
	abstract = {An ambitious goal for artificial intelligence is to create agents that behave ethically: The capacity to abide by human moral norms would greatly expand the context in which autonomous agents could be practically and safely deployed. While ethical agents could be trained through reinforcement, by rewarding correct behavior under a specific moral theory (e.g. utilitarianism), there remains widespread disagreement (both societally and among moral philosophers) about the nature of morality and what ethical theory (if any) is objectively correct. Acknowledging such disagreement, recent work in moral philosophy proposes that ethical behavior requires acting under moral uncertainty, i.e. to take into account when acting that one's credence is split across several plausible ethical theories. Inspired by such work, this paper proposes a formalism that translates such insights to the field of reinforcement learning. Demonstrating the formalism's potential, we then train agents in simple environments to act under moral uncertainty, highlighting how such uncertainty can help curb extreme behavior from commitment to single theories. The overall aim is to draw productive connections from the fields of moral philosophy and machine ethics to that of machine learning, to inspire further research by highlighting a spectrum of machine learning research questions relevant to training ethically capable reinforcement learning agents.},
	urldate = {2021-03-06},
	journal = {arXiv:2006.04734 [cs]},
	author = {Ecoffet, Adrien and Lehman, Joel},
	month = jul,
	year = {2020},
	note = {arXiv: 2006.04734},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 33 pages, 17 figures; update adds discussion of a possible flaw of Nash voting, discussion of further possible research into MEC, as well as a few more references}
}

@article{vamplew_demonstration_2020,
	title = {A {Demonstration} of {Issues} with {Value}-{Based} {Multiobjective} {Reinforcement} {Learning} {Under} {Stochastic} {State} {Transitions}},
	url = {http://arxiv.org/abs/2004.06277},
	abstract = {We report a previously unidentified issue with model-free, value-based approaches to multiobjective reinforcement learning in the context of environments with stochastic state transitions. An example multiobjective Markov Decision Process (MOMDP) is used to demonstrate that under such conditions these approaches may be unable to discover the policy which maximises the Scalarised Expected Return, and in fact may converge to a Pareto-dominated solution. We discuss several alternative methods which may be more suitable for maximising SER in MOMDPs with stochastic transitions.},
	urldate = {2021-03-06},
	journal = {arXiv:2004.06277 [cs, stat]},
	author = {Vamplew, Peter and Foale, Cameron and Dazeley, Richard},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.06277},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Multiagent Systems},
	annote = {Comment: 6 pages. Accepted for presentation in the Adaptive and Learning Agents Workshop, AAMAS 2020}
}

@article{omohundro_nature_nodate,
	title = {The {Nature} of {Self}-{Improving} {Artiﬁcial} {Intelligence}},
	language = {en},
	author = {Omohundro, Stephen M and Systems, Self-Aware},
	pages = {48}
}

@article{leike_scalable_2018,
	title = {Scalable agent alignment via reward modeling: a research direction},
	shorttitle = {Scalable agent alignment via reward modeling},
	url = {http://arxiv.org/abs/1811.07871},
	abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
	urldate = {2021-03-06},
	journal = {arXiv:1811.07871 [cs, stat]},
	author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.07871},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{bieger_safe_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Safe {Baby} {AGI}},
	isbn = {978-3-319-21365-1},
	doi = {10.1007/978-3-319-21365-1_5},
	abstract = {Out of fear that artificial general intelligence (AGI) might pose a future risk to human existence, some have suggested slowing or stopping AGI research, to allow time for theoretical work to guarantee its safety. Since an AGI system will necessarily be a complex closed-loop learning controller that lives and works in semi-stochastic environments, its behaviors are not fully determined by its design and initial state, so no mathematico-logical guarantees can be provided for its safety. Until actual running AGI systems exist – and there is as of yet no consensus on how to create them – that can be thoroughly analyzed and studied, any proposal on their safety can only be based on weak conjecture. As any practical AGI will unavoidably start in a relatively harmless baby-like state, subject to the nurture and education that we provide, we argue that our best hope to get safe AGI is to provide it proper education.},
	language = {en},
	booktitle = {Artificial {General} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Bieger, Jordi and Thórisson, Kristinn R. and Wang, Pei},
	editor = {Bieger, Jordi and Goertzel, Ben and Potapov, Alexey},
	year = {2015},
	keywords = {Artificial intelligence, AI safety, Friendly AI, Nature, Nurture},
	pages = {46--49}
}

@article{bogosian_implementation_2017,
	title = {Implementation of {Moral} {Uncertainty} in {Intelligent} {Machines}},
	volume = {27},
	issn = {1572-8641},
	url = {https://doi.org/10.1007/s11023-017-9448-z},
	doi = {10.1007/s11023-017-9448-z},
	abstract = {The development of artificial intelligence will require systems of ethical decision making to be adapted for automatic computation. However, projects to implement moral reasoning in artificial moral agents so far have failed to satisfactorily address the widespread disagreement between competing approaches to moral philosophy. In this paper I argue that the proper response to this situation is to design machines to be fundamentally uncertain about morality. I describe a computational framework for doing so and show that it efficiently resolves common obstacles to the implementation of moral philosophy in intelligent machines.},
	language = {en},
	number = {4},
	urldate = {2021-03-06},
	journal = {Minds and Machines},
	author = {Bogosian, Kyle},
	month = dec,
	year = {2017},
	pages = {591--608}
}

@inproceedings{martinho_empirical_2020,
	title = {An {Empirical} {Approach} to {Capture} {Moral} {Uncertainty} in {AI}},
	booktitle = {Proceedings of the {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	author = {Martinho, Andreia and Kroesen, Maarten and Chorus, Caspar},
	year = {2020},
	pages = {101--101}
}

@article{dewey_reinforcement_nodate,
	title = {Reinforcement {Learning} and the {Reward} {Engineering} {Principle}},
	abstract = {AI agents are becoming signiﬁcantly more general and autonomous. We argue for the “Reward Engineering Principle”: as reinforcement-learning-based AI systems, become more general and autonomous, the design of reward mechanisms that elicit desired behaviours becomes both more important and more difﬁcult. While early AI research could ignore reward design and focus solely on the problems of efﬁcient, ﬂexible, and effective achievement of arbitrary goals in varied environments, the reward engineering principle will affect modern AI research, both theoretical and applied, in the medium and long terms. We introduce some notation and derive preliminary results that formalize the intuitive landmarks of the area of reward design.},
	language = {en},
	author = {Dewey, Daniel},
	pages = {4}
}

@article{roijers_survey_2013,
	title = {A {Survey} of {Multi}-{Objective} {Sequential} {Decision}-{Making}},
	volume = {48},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1402.0590},
	doi = {10.1613/jair.3987},
	abstract = {Sequential decision-making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision-theoretic planning and learning, which has largely focused on single-objective settings. This article surveys algorithms designed for sequential decision-making problems with multiple objectives. Though there is a growing body of literature on this subject, little of it makes explicit under what circumstances special methods are needed to solve multi-objective problems. Therefore, we identify three distinct scenarios in which converting such a problem to a single-objective one is impossible, infeasible, or undesirable. Furthermore, we propose a taxonomy that classifies multi-objective methods according to the applicable scenario, the nature of the scalarization function (which projects multi-objective values to scalar ones), and the type of policies considered. We show how these factors determine the nature of an optimal solution, which can be a single policy, a convex hull, or a Pareto front. Using this taxonomy, we survey the literature on multi-objective methods for planning and learning. Finally, we discuss key applications of such methods and outline opportunities for future work.},
	urldate = {2021-03-06},
	journal = {Journal of Artificial Intelligence Research},
	author = {Roijers, Diederik Marijn and Vamplew, Peter and Whiteson, Shimon and Dazeley, Richard},
	month = oct,
	year = {2013},
	note = {arXiv: 1402.0590},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {67--113}
}


@article{taylor_quantilizers_nodate,
	title = {Quantilizers: {A} {Safer} {Alternative} to {Maximizers} for {Limited} {Optimization}},
	abstract = {In the ﬁeld of AI, expected utility maximizers are commonly used as a model for idealized agents. However, expected utility maximization can lead to unintended solutions when the utility function does not quantify everything the operators care about: imagine, for example, an expected utility maximizer tasked with winning money on the stock market, which has no regard for whether it accidentally causes a market crash. Once AI systems become suﬃciently intelligent and powerful, these unintended solutions could become quite dangerous. In this paper, we describe an alternative to expected utility maximization for powerful AI systems, which we call expected utility quantilization. This could allow the construction of AI systems that do not necessarily fall into strange and unanticipated shortcuts and edge cases in pursuit of their goals.},
	language = {en},
	author = {Taylor, Jessica},
	pages = {9}
}

@inproceedings{soares_corrigibility_2015,
	title = {Corrigibility},
	url = {https://openreview.net/forum?id=H1bIT1buWH},
	abstract = {As artificially intelligent systems grow in intelligence and capability, some of their available options may allow them to resist intervention by their programmers. We call an AI system...},
	language = {en},
	urldate = {2021-03-06},
	author = {Soares, Nate and Fallenstein, Benja and Armstrong, Stuart and Yudkowsky, Eliezer},
	month = jan,
	year = {2015}
}

@article{oesterheld_backup_2016,
	title = {Backup utility functions as a fail-safe {AI} technique},
	volume = {2},
	journal = {Foundational Research Institute, Report FRI-16},
	author = {Oesterheld, C.},
	year = {2016}
}

@article{gloor_suffering-focused_2016,
	title = {Suffering-focused {AI} safety: {Why} “fail-safe” measures might be our top intervention},
	volume = {1},
	shorttitle = {Suffering-focused {AI} safety},
	journal = {Foundational Research Institute, Report FRI-16},
	author = {Gloor, Lukas},
	year = {2016}
}

@misc{krakovna_specification_2018,
	title = {Specification gaming examples in {AI}},
	url = {https://vkrakovna.wordpress.com/2018/04/02/specification-gaming-examples-in-ai/},
	abstract = {Update: for a more detailed introduction to specification gaming, check out the DeepMind Safety Research blog post! Various examples (and lists of examples) of unintended behaviors in AI systems ha…},
	language = {en},
	urldate = {2021-03-07},
	journal = {Victoria Krakovna},
	author = {Krakovna, Victoria},
	month = apr,
	year = {2018}
}

@article{amodei_concrete_2016,
	title = {Concrete {Problems} in {AI} {Safety}},
	url = {http://arxiv.org/abs/1606.06565},
	abstract = {Rapid progress in machine learning and artiﬁcial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, deﬁned as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of ﬁve practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (“avoiding side eﬀects” and “avoiding reward hacking”), an objective function that is too expensive to evaluate frequently (“scalable supervision”), or undesirable behavior during the learning process (“safe exploration” and “distributional shift”). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	language = {en},
	urldate = {2020-11-07},
	journal = {arXiv:1606.06565 [cs]},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jul,
	year = {2016},
	note = {arXiv: 1606.06565},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 29 pages}
}

@article{alfonseca_superintelligence_2021,
	title = {Superintelligence cannot be contained: {Lessons} from {Computability} {Theory}},
	volume = {70},
	issn = {1076-9757},
	shorttitle = {Superintelligence cannot be contained},
	url = {http://arxiv.org/abs/1607.00913},
	doi = {10.1613/jair.1.12202},
	abstract = {Superintelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. In light of recent advances in machine intelligence, a number of scientists, philosophers and technologists have revived the discussion about the potential catastrophic risks entailed by such an entity. In this article, we trace the origins and development of the neo-fear of superintelligence, and some of the major proposals for its containment. We argue that such containment is, in principle, impossible, due to fundamental limits inherent to computing itself. Assuming that a superintelligence will contain a program that includes all the programs that can be executed by a universal Turing machine on input potentially as complex as the state of the world, strict containment requires simulations of such a program, something theoretically (and practically) infeasible.},
	urldate = {2021-03-07},
	journal = {Journal of Artificial Intelligence Research},
	author = {Alfonseca, Manuel and Cebrian, Manuel and Anta, Antonio Fernandez and Coviello, Lorenzo and Abeliuk, Andres and Rahwan, Iyad},
	month = jan,
	year = {2021},
	note = {arXiv: 1607.00913},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	pages = {65--76},
	annote = {Comment: 7 pages, 5 figures}
}

@article{krakovna_avoiding_2020,
	title = {Avoiding {Side} {Effects} {By} {Considering} {Future} {Tasks}},
	url = {http://arxiv.org/abs/2010.07877},
	abstract = {Designing reward functions is difficult: the designer has to specify what to do (what it means to complete the task) as well as what not to do (side effects that should be avoided while completing the task). To alleviate the burden on the reward designer, we propose an algorithm to automatically generate an auxiliary reward function that penalizes side effects. This auxiliary objective rewards the ability to complete possible future tasks, which decreases if the agent causes side effects during the current task. The future task reward can also give the agent an incentive to interfere with events in the environment that make future tasks less achievable, such as irreversible actions by other agents. To avoid this interference incentive, we introduce a baseline policy that represents a default course of action (such as doing nothing), and use it to filter out future tasks that are not achievable by default. We formally define interference incentives and show that the future task approach with a baseline policy avoids these incentives in the deterministic case. Using gridworld environments that test for side effects and interference, we show that our method avoids interference and is more effective for avoiding side effects than the common approach of penalizing irreversible actions.},
	urldate = {2021-03-07},
	journal = {arXiv:2010.07877 [cs]},
	author = {Krakovna, Victoria and Orseau, Laurent and Ngo, Richard and Martic, Miljan and Legg, Shane},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.07877},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Published in NeurIPS 2020}
}

@article{turner_conservative_2020,
	title = {Conservative {Agency} via {Attainable} {Utility} {Preservation}},
	url = {http://arxiv.org/abs/1902.09725},
	doi = {10.1145/3375627.3375851},
	abstract = {Reward functions are easy to misspecify; although designers can make corrections after observing mistakes, an agent pursuing a misspeciﬁed reward function can irreversibly change the state of its environment. If that change precludes optimization of the correctly speciﬁed reward function, then correction is futile. For example, a robotic factory assistant could break expensive equipment due to a reward misspeciﬁcation; even if the designers immediately correct the reward function, the damage is done. To mitigate this risk, we introduce an approach that balances optimization of the primary reward function with preservation of the ability to optimize auxiliary reward functions. Surprisingly, even when the auxiliary reward functions are randomly generated and therefore uninformative about the correctly speciﬁed reward function, this approach induces conservative, effective behavior.},
	language = {en},
	urldate = {2021-03-07},
	journal = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Turner, Alexander Matt and Hadfield-Menell, Dylan and Tadepalli, Prasad},
	month = feb,
	year = {2020},
	note = {arXiv: 1902.09725},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {385--391},
	annote = {Comment: Published in AI, Ethics, and Society 2020}
}

@article{christiano_supervising_2018,
	title = {Supervising strong learners by amplifying weak experts},
	url = {http://arxiv.org/abs/1810.08575},
	abstract = {Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.},
	urldate = {2021-03-07},
	journal = {arXiv:1810.08575 [cs, stat]},
	author = {Christiano, Paul and Shlegeris, Buck and Amodei, Dario},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.08575},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{armstrong_indifference_2018,
	title = {'{Indifference}' methods for managing agent rewards},
	url = {http://arxiv.org/abs/1712.06365},
	abstract = {`Indifference' refers to a class of methods used to control reward based agents. Indifference techniques aim to achieve one or more of three distinct goals: rewards dependent on certain events (without the agent being motivated to manipulate the probability of those events), effective disbelief (where agents behave as if particular events could never happen), and seamless transition from one reward function to another (with the agent acting as if this change is unanticipated). This paper presents several methods for achieving these goals in the POMDP setting, establishing their uses, strengths, and requirements. These methods of control work even when the implications of the agent's reward are otherwise not fully understood.},
	urldate = {2021-03-07},
	journal = {arXiv:1712.06365 [cs]},
	author = {Armstrong, Stuart and O'Rourke, Xavier},
	month = jun,
	year = {2018},
	note = {arXiv: 1712.06365},
	keywords = {Computer Science - Artificial Intelligence}
}


@article{krakovna_penalizing_2019,
	title = {Penalizing side effects using stepwise relative reachability},
	url = {http://arxiv.org/abs/1806.01186},
	abstract = {How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.},
	urldate = {2021-03-07},
	journal = {arXiv:1806.01186 [cs, stat]},
	author = {Krakovna, Victoria and Orseau, Laurent and Kumar, Ramana and Martic, Miljan and Legg, Shane},
	month = mar,
	year = {2019},
	note = {arXiv: 1806.01186},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{critch_toward_2017,
	title = {Toward negotiable reinforcement learning: shifting priorities in {Pareto} optimal sequential decision-making},
	shorttitle = {Toward negotiable reinforcement learning},
	journal = {arXiv preprint arXiv:1701.01302},
	author = {Critch, Andrew},
	year = {2017}
}

@article{critch_toward_2017-1,
	title = {Toward negotiable reinforcement learning: shifting priorities in {Pareto} optimal sequential decision-making},
	shorttitle = {Toward negotiable reinforcement learning},
	journal = {arXiv preprint arXiv:1701.01302},
	author = {Critch, Andrew},
	year = {2017}
}

@misc{noauthor_elicit_nodate,
	title = {Elicit},
	url = {https://elicit.org/stages/1},
	abstract = {The AI research assistant},
	urldate = {2021-03-27}
}

@inproceedings{bobu_learning_2018,
	title = {Learning under misspecified objective spaces},
	booktitle = {Conference on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Bobu, Andreea and Bajcsy, Andrea and Fisac, Jaime F. and Dragan, Anca D.},
	year = {2018},
	pages = {796--805}
}

@misc{noauthor_sarah_nodate,
	title = {Sarah {Jeong} out at {New} {York} {Times} editorial board {\textbar} {TheHill}},
	url = {https://thehill.com/homenews/media/463503-sarah-jeong-out-at-new-york-times-editorial-board},
	urldate = {2021-04-10}
}

@inproceedings{saisubramanian_multi-objective_2020,
	title = {A {Multi}-{Objective} {Approach} to {Mitigate} {Negative} {Side} {Effects}},
	volume = {1},
	url = {https://www.ijcai.org/proceedings/2020/50},
	doi = {10.24963/ijcai.2020/50},
	abstract = {Electronic proceedings of IJCAI 2020},
	language = {en},
	urldate = {2021-04-19},
	author = {Saisubramanian, Sandhya and Kamar, Ece and Zilberstein, Shlomo},
	month = jul,
	year = {2020},
	note = {ISSN: 1045-0823},
	pages = {354--361}
}
